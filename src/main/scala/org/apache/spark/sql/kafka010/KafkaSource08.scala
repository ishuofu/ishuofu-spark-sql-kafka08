package org.apache.spark.sql.kafka010

import java.io.{BufferedWriter, InputStream, InputStreamReader, OutputStream, OutputStreamWriter}
import java.nio.charset.StandardCharsets

import kafka.serializer.DefaultDecoder
import org.apache.commons.io.IOUtils
import org.apache.kafka.common.TopicPartition
import org.apache.spark.SparkException
import org.apache.spark.internal.Logging
import org.apache.spark.sql._
import org.apache.spark.sql.catalyst.InternalRow
import org.apache.spark.sql.execution.streaming._
import org.apache.spark.sql.kafka010.KafkaSource.VERSION
import org.apache.spark.sql.types._
import org.apache.spark.streaming.kafka.{KafkaCluster, KafkaUtils, OffsetRange}

import scala.annotation.tailrec

case class KafkaSource08(
                          sqlContext: SQLContext,
                          topics: Set[String],
                          kafkaParams: Map[String, String],
                          sourceOptions: Map[String, String],
                          metadataPath: String,
                          startingOffsets: KafkaOffsetRangeLimit
                        )
  extends Source with Logging {

  private val sc = sqlContext.sparkContext
  private val kc = new KafkaCluster(kafkaParams)
  private val topicPartitions = KafkaCluster.checkErrors(kc.getPartitions(topics))
  private val maxOffsetFetchAttempts = sourceOptions.getOrElse("fetchOffset.numRetries", "3").toInt
  private val maxOffsetsPerTrigger = sourceOptions.get("maxOffsetsPerTrigger").map(_.toLong)

  private lazy val initialPartitionOffsets = {
    val metadataLog =
      new HDFSMetadataLog[KafkaSourceOffset](sqlContext.sparkSession, metadataPath) {
        override def serialize(metadata: KafkaSourceOffset, out: OutputStream): Unit = {
          out.write(0) // A zero byte is written to support Spark 2.1.0 (SPARK-19517)
          val writer = new BufferedWriter(new OutputStreamWriter(out, StandardCharsets.UTF_8))
          writer.write("v" + VERSION + "\n")
          writer.write(metadata.json)
          writer.flush
        }

        override def deserialize(in: InputStream): KafkaSourceOffset = {
          in.read() // A zero byte is read to support Spark 2.1.0 (SPARK-19517)
          val content = IOUtils.toString(new InputStreamReader(in, StandardCharsets.UTF_8))
          // HDFSMetadataLog guarantees that it never creates a partial file.
          assert(content.length != 0)
          if (content(0) == 'v') {
            val indexOfNewLine = content.indexOf("\n")
            if (indexOfNewLine > 0) {
              val version = parseVersion(content.substring(0, indexOfNewLine), VERSION)
              KafkaSourceOffset(SerializedOffset(content.substring(indexOfNewLine + 1)))
            } else {
              throw new IllegalStateException(
                s"Log file was malformed: failed to detect the log file version line.")
            }
          } else {
            // The log was generated by Spark 2.1.0
            KafkaSourceOffset(SerializedOffset(content))
          }
        }
      }

    metadataLog.get(0).getOrElse {
      val offsets = startingOffsets match {
        case EarliestOffsetRangeLimit => KafkaSourceOffset(fetchEarliestOffsets(maxOffsetFetchAttempts))
        case LatestOffsetRangeLimit => KafkaSourceOffset(fetchLatestOffsets(maxOffsetFetchAttempts))
        case _=>throw new UnsupportedOperationException
      }
      metadataLog.add(0, offsets)
      logInfo(s"Initial offsets: $offsets")
      offsets
    }.partitionToOffsets
  }

  private var currentPartitionOffsets: Option[Map[TopicPartition, Long]] = None

  override def schema: StructType = KafkaSource08.kafkaSchema

  override def getOffset: Option[Offset] = {
    // Make sure initialPartitionOffsets is initialized
    initialPartitionOffsets

    val latest = fetchLatestOffsets(maxOffsetFetchAttempts)
    val offsets = maxOffsetsPerTrigger match {
      case None =>
        latest
      case Some(limit) if currentPartitionOffsets.isEmpty =>
        rateLimit(limit, initialPartitionOffsets, latest)
      case Some(limit) =>
        rateLimit(limit, currentPartitionOffsets.get, latest)
    }

    currentPartitionOffsets = Some(offsets)
    logDebug(s"GetOffset: ${offsets.toSeq.map(_.toString).sorted}")
    Some(KafkaSourceOffset(offsets))
  }

  private def rateLimit(
                         limit: Long,
                         from: Map[TopicPartition, Long],
                         until: Map[TopicPartition, Long]): Map[TopicPartition, Long] = {
    val fromNew = fetchEarliestOffsets(maxOffsetFetchAttempts)
    val sizes = until.flatMap {
      case (tp, end) =>
        // If begin isn't defined, something's wrong, but let alert logic in getBatch handle it
        from.get(tp).orElse(fromNew.get(tp)).flatMap { begin =>
          val size = end - begin
          logDebug(s"rateLimit $tp size is $size")
          if (size > 0) Some(tp -> size) else None
        }
    }
    val total = sizes.values.sum.toDouble
    if (total < 1) {
      until
    } else {
      until.map {
        case (tp, end) =>
          tp -> sizes.get(tp).map { size =>
            val begin = from.get(tp).getOrElse(fromNew(tp))
            val prorate = limit * (size / total)
            logDebug(s"rateLimit $tp prorated amount is $prorate")
            // Don't completely starve small topicpartitions
            val off = begin + (if (prorate < 1) Math.ceil(prorate) else Math.floor(prorate)).toLong
            logDebug(s"rateLimit $tp new offset is $off")
            // Paranoia, make sure not to return an offset that's past end
            Math.min(end, off)
          }.getOrElse(end)
      }
    }
  }

  override def getBatch(start: Option[Offset], end: Offset): DataFrame = {
    // Make sure initialPartitionOffsets is initialized
    initialPartitionOffsets

    logInfo(s"GetBatch called with start = $start, end = $end")
    val untilPartitionOffsets = KafkaSourceOffset.getPartitionOffsets(end)
    // On recovery, getBatch will get called before getOffset
    if (currentPartitionOffsets.isEmpty) {
      currentPartitionOffsets = Some(untilPartitionOffsets)
    }
    if (start.isDefined && start.get == end) {
      return sqlContext.internalCreateDataFrame(
        sqlContext.sparkContext.emptyRDD, schema, isStreaming = true)
    }
    val fromPartitionOffsets = start match {
      case Some(prevBatchEndOffset) =>
        KafkaSourceOffset.getPartitionOffsets(prevBatchEndOffset)
      case None =>
        initialPartitionOffsets
    }

    val offsetRanges = fromPartitionOffsets.map { case (tp, fo) =>
      val uo = untilPartitionOffsets(tp)
      OffsetRange(tp.topic, tp.partition, fo, uo)
    }.filter { range =>
      if (range.untilOffset < range.fromOffset) {
        false
      } else {
        true
      }
    }.toArray

    val rdd = KafkaUtils.createRDD[
      Array[Byte],
      Array[Byte],
      DefaultDecoder,
      DefaultDecoder](sc, kafkaParams, offsetRanges)
      .map(r => InternalRow(r._2))

    logInfo("GetBatch generating RDD of offset range: " + offsetRanges.sortBy(_.topic).mkString(","))
    sqlContext.internalCreateDataFrame(rdd, schema, true)
  }

  /** Stop this source and free any resources it has allocated. */
  override def stop(): Unit = {}

  @tailrec
  private def fetchEarliestOffsets(retries: Int): Map[TopicPartition, Long] = {
    val offsets = kc.getEarliestLeaderOffsets(topicPartitions)
    if (offsets.isLeft) {
      val err = offsets.left.get.toString
      if (retries <= 0) {
        throw new SparkException(err)
      } else {
        error(err)
        Thread.sleep(kc.config.refreshLeaderBackoffMs)
        fetchEarliestOffsets(retries - 1)
      }
    } else {
      offsets.right.get.map({ case (tp, lo) => (new TopicPartition(tp.topic, tp.partition), lo.offset) })
    }
  }

  @tailrec
  private def fetchLatestOffsets(retries: Int): Map[TopicPartition, Long] = {
    val offsets = kc.getLatestLeaderOffsets(topicPartitions)
    if (offsets.isLeft) {
      val err = offsets.left.get.toString
      if (retries <= 0) {
        throw new SparkException(err)
      } else {
        error(err)
        Thread.sleep(kc.config.refreshLeaderBackoffMs)
        fetchLatestOffsets(retries - 1)
      }
    } else {
      offsets.right.get.map({ case (tp, lo) => (new TopicPartition(tp.topic, tp.partition), lo.offset) })
    }
  }
}

private[kafka010] object KafkaSource08 {

  def kafkaSchema: StructType = StructType(Seq(
    StructField("value", BinaryType)
  ))
}